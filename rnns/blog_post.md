---
title: Recurrent Neural Networks
published: false
description: ???
tags: ???
cover_image: ???
---

# Introduction

- "RNNs are a powerful set of artificial neural network algorithms especially useful for processing sequential data such as sound, time series (sensor) data or written natural language."
- "Recurrent nets differ from feedforward nets because they include a feedback loop, whereby output from step n-1 is fed back to the net to affect the outcome of step n, and so forth for each subsequent step. For example, if a net is exposed to a word letter by letter, and it is asked to guess each following letter, the first letter of a word will help determine what a recurrent net thinks the second letter will be, etc."
- "This differs from a feedforward network, which learns to classify each handwritten numeral of MNIST independently according to the pixels it is exposed to from a single example, without referring to the preceding example to adjust its predictions. Feedforward networks accept one input at a time, and produce one output. Recurrent nets don’t face the same one-to-one constraint."
- "Recurrent nets and feedforward nets both “remember” something about the world, in a loose sense, by modeling the data they are exposed to. But they remember in very different ways. After training, feedforward net produces a static model of the data it has been shown, and that model can then accept new examples and accurately classify or cluster them. In contrast, recurrent nets produce dynamic models – i.e. models that change over time – in ways that yield accurate classifications dependent of the context of the examples they’re exposed to."
- "To be precise, recurrent models include the hidden state that determined the previous classification in a series. In each subsequent step, that hidden state is combined with the new step’s input data to produce a) a new hidden state and then b) a new classification. Each hidden state is recycled to produce its modified successor." => similar to hidden markov model? https://www.quora.com/What-are-differences-between-recurrent-neural-network-language-model-hidden-markov-model-and-n-gram-language-model

- "The recurrent net that effectively associates memories and input remote in time is called a Long Short-Term Memory (LSTM), as much as that sounds like an oxymoron."

# Use Cases

"Recurrent nets have predictive capacity. They grasp the structure of data dynamically over time, and they are used to predict the next element in a series. Those elements might be the next letters in a word, or the next words in a sentence (natural language generation); the next number in data from sensors, economic tables, stock price action, etc.

Sequential data also includes videos, and recurrent networks have been used for object and gesture tracking in videos in real-time.

Recurrent nets, like other neural nets, are useful for clustering and anomaly detection. That is, they recognize similarities and dissimilarities by grouping examples in vector space and measuring their distance from each other. Modeling normal behavior, and flagging abnormalities, is applicable to healthcare data generated by wearables; home data generated by smart objects such as thermostats; market data generated by the movement of stocks and indices; personal financial data generated by account transactions (which may be used to identify fraud and money laundering)."

# Types

- Vanilla RNN (with tanh, one hidden state)
- Multilayer RNNs
- LSTMs (tackling the problem of vanishing or exploding gradients)

## LSTMs

- cell states are like little incremental integer counters, counting -1 or +1 respectively on every step
- https://youtu.be/6niqTuYFZLQ?t=3603

# Training

- Truncated backpropagation through time corresponds to stochastic gradient descent

# Code

https://github.com/deeplearning4j/dl4j-examples/blob/master/dl4j-examples/src/main/java/org/deeplearning4j/examples/recurrent/character/LSTMCharModellingExample.java
