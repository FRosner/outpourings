---
title: Continuous Delivery on AWS With Terraform and Travis CI
published: false
description:
tags: aws, cloud, devops, travis
cover_image:
---

This blog post is part of my AWS series:

- [Infrastructure as Code - Managing AWS With Terraform](https://dev.to/frosnerd/infrastructure-as-code---managing-aws-with-terraform-i9o)
- [Deploying an HTTP API on AWS using Lambda and API Gateway](https://dev.to/frosnerd/deploying-an-http-api-on-aws-using-lambda-and-api-gateway-g61)
- [Deploying an HTTP API on AWS using Elastic Beanstalk](https://dev.to/frosnerd/deploying-an-http-api-on-aws-using-elastic-beanstalk-5dh7)
- [Deploying and Benchmarking an AWS RDS MySQL Instance](https://dev.to/frosnerd/deploying-and-benchmarking-an-aws-rds-mysql-instance-2faf)
- [Event Handling in AWS using SNS, SQS, and Lambda](https://dev.to/frosnerd/event-handling-in-aws-using-sns-sqs-and-lambda-2ng)
- [**Continuous Delivery on AWS With Terraform and Travis CI**](#)

# Introduction

In the previous posts we introduced and extensively used Terraform for automating infrastructure deployments. If you are aiming at true continuous delivery a high degree of automation is crucial. Continuous delivery (CD) is about producing software in short cycles with high confidence, reducing the risk of delivering changes.

In this blog post we want to combine Terraform with an automated build pipeline on [Travis CI](https://travis-ci.org/). In order to use Terraform in a shared setting we have to configure Terraform to use [remote state](https://www.terraform.io/docs/state/remote.html), as local state cannot be used for any project which involves multiple developers or automated build pipelines. The application we are deploying will be a static website generated by [Jekyll](https://jekyllrb.com/).

The remainder of the post is structured as follows. [TODO]

# Architecture

![architecture overview](https://thepracticaldev.s3.amazonaws.com/i/m1abt2656tiyxybgpdy9.png)

The above figure visualizes the solution architecture including the components for continuous integration (CI) and CD. The client is the developer in this case as we are looking at the setup from the development point of view.

As soon as a developer pushes new changes to the remote GitHub repository it triggers a Travis CI build. Travis CI is a hosted build service that is free to use for open source projects. Travis then builds the website artifacts, deploys the infrastructure, and pushes the artifacts to production.

We are using an [S3 backend](https://www.terraform.io/docs/backends/types/s3.html) with [DynamoDB](http://aws.amazon.com/dynamodb) for Terraform. Terraform will store the state within S3 and use DynamoDB to acquire a lock while performing changes. The lock is important to avoid that two Terraform binaries are modifying the same state concurrently.

To use the S3 remote state backend we need to create the S3 bucket and DynamoDB table beforehand. This bootstrapping is also done and automated with Terraform. But how do we manage infrastructure with Terraform that is required to use Terraform? The next section will discuss two approaches to solve this ðŸ” & ðŸ¥š problem.

# Remote State Chicken And Egg Problem

How can we use Terraform to setup the S3 bucket and DynamoDB table we want to use for the remote state backend? First we create the remote backend resources with local state. Then we somehow need to share this state to allow modifications of the backend resources later on. From what I can tell there are two viable solutions to do that:

1. **Shared local state.** Commit local state to your version control and share it in a remote repository.
2. **Migrated state.** Migrate local state to remote state backend.

Both solutions involve creating the remote state resources using local state. They differ in the way how the state for provisioning the remote state resources is shared. While the first option is easy to setup there are two major risks that need to be taken into account:

- **Terraform state might contain secrets.** In the case of only the S3 bucket and DynamoDB table there is only one variable which might be problematic: The AWS access key. If you are working with a private repository, this might not be a huge issue. When working on open source code it might be useful to encrypt the state file before committing it. You can do this with [OpenSSL](https://www.openssl.org/) or more specialized tools like [Ansible Vault](https://docs.ansible.com/ansible/2.4/vault.html).
- **Shared local state has no locking or synchronization mechanism.** When publishing your local Terraform state to the remote source code repository you have to manually make sure to keep this state file in sync with all developers. If someone is making modifications to the resources he has to commit and push the updated state file and make sure that no one else is modifying the infrastructure at the same time.

The second option is a bit safer with regards to the above-mentioned issues. S3 supports encryption at rest out of the box and you can have fine granular access control on the bucket. Also as DynamoDB is used for locking, two parties cannot modify the resources concurrently. The disadvantage is that the solution is more complex.

After we migrate the local state to the created remote state backend, it will contain the state for the backend itself plus the application infrastructure state. Luckily Terraform provides a built-in way to isolate state of different environments: [Workspaces](https://www.terraform.io/docs/state/workspaces.html). We can create a separate workspace for the backend resources to avoid interference between changes in our backend infrastructure and application infrastructure.

Working with workspaces is a bit difficult to wrap your head around so we are going to utilize the second option in the course of this post. In practice I am not sure if the increased complexity is worth the effort, especially as you usually do not touch the backend infrastructure unless you want to shut down the project. The next section will explain the bootstrapping and application implementation and deployment step by step.

# Implementation

## Development Tool Stack

To develop the solution we are using the following tools:

- Terraform v0.11.7
- Jekyll 3.8.3
- Git 2.15.2
- IntelliJ + Terraform Plugin

The [source code](https://github.com/FRosner/aws_travis) is available on GitHub. Now let's look into the implementation details of each component.

## Remote State Bootstrapping And Configuration

We will organize our Terraform files in workspaces and folders. Workspaces isolate the backend resource state from the application resource state. Folders will be used to organize the Terraform resource files.

We will create two workspaces: `state` and `prod`. The `state` workspace will manage the remote state resources, i.e. the S3 bucket and the DynamoDB table. The `prod` workspace will manage the production environment of our website. You can add more workspaces for staging or testing later but this is beyond the scope of this blog post.

We will create three folders containing Terraform files: `bootstrap`, `backend`, and `website`. The next listing outlines the directory and file structure of the project.

```
.
â”œâ”€â”€ locals.tf
â”œâ”€â”€ providers.tf
â”œâ”€â”€ backend
â”‚Â Â  â”œâ”€â”€ backend.tf
â”‚Â Â  â”œâ”€â”€ backend.tf.tmpl
â”‚Â Â  â”œâ”€â”€ locals.tf -> ../locals.tf
â”‚Â Â  â”œâ”€â”€ providers.tf -> ../providers.tf
â”‚Â Â  â””â”€â”€ state.tf -> ../bootstrap/state.tf
â”œâ”€â”€ bootstrap
â”‚Â Â  â”œâ”€â”€ locals.tf -> ../locals.tf
â”‚Â Â  â”œâ”€â”€ providers.tf -> ../providers.tf
â”‚Â Â  â””â”€â”€ state.tf
â””â”€â”€ website
    â”œâ”€â”€ backend.tf -> ../backend/backend.tf
    â”œâ”€â”€ locals.tf -> ../locals.tf
    â”œâ”€â”€ providers.tf -> ../providers.tf
    â””â”€â”€ website.tf
```

The project root will contain a shared AWS provider configuration `providers.tf`, as well as a project name variable inside `locals.tf`. We will go into details about the file contents later.

In addition to the shared files `bootstrap` contains `state.tf`, which defines the S3 bucket and DynamoDB table backend resources. We share them across folders using symbolic links. The `backend` folder will have the same resources but uses the already present S3 backend defined in `backend.tf`. When switching from `bootstrap` to `backend` after the initial provisioning, Terraform will migrate the local state to the remote backend.

The `website` folder contains the remote backend configuration and all resources related to the actual website deployment. We will access `backend` and `bootstrap` from the `state` workspace and `website` from `prod` and any other additional workspace related to the application.

The next listing shows what the [`state.tf`](https://github.com/FRosner/aws_travis/blob/d1b5241e5fad4f1356a761cf3c9299968c4006c8/bootstrap/state.tf) file looks like. The `project_name` local variable is defined within the shared [`locals.tf`](https://github.com/FRosner/aws_travis/blob/d1b5241e5fad4f1356a761cf3c9299968c4006c8/locals.tf) file. The current `aws_caller_identity` and `aws_region` are defined within the shared [`providers.tf`](https://github.com/FRosner/aws_travis/blob/d1b5241e5fad4f1356a761cf3c9299968c4006c8/providers.tf) file.

```conf
# state.tf

locals {
  state_bucket_name = "${local.project_name}-${data.aws_caller_identity.current.account_id}-${data.aws_region.current.name}"
  state_table_name = "${local.state_bucket_name}"
}

resource "aws_dynamodb_table" "locking" {
  name           = "${local.state_table_name}"
  read_capacity  = "20"
  write_capacity = "20"
  hash_key       = "LockID"

  attribute {
    name = "LockID"
    type = "S"
  }
}

resource "aws_s3_bucket" "state" {
  bucket = "${local.state_bucket_name}"
  region = "${data.aws_region.current.name}"

  versioning {
    enabled = true
  }

  server_side_encryption_configuration {
    "rule" {
      "apply_server_side_encryption_by_default" {
        sse_algorithm = "AES256"
      }
    }
  }

  tags {
    Name = "terraform-state-bucket"
    Environment = "global"
    project = "${local.project_name}"
  }
}

output "BACKEND_BUCKET_NAME" {
  value = "${aws_s3_bucket.state.bucket}"
}

output "BACKEND_TABLE_NAME" {
  value = "${aws_dynamodb_table.locking.name}"
}
```

It defines the [TODO]

```
$ terraform init backend

Initializing the backend...
Do you want to migrate all workspaces to "s3"?
  Both the existing "local" backend and the newly configured "s3" backend support
  workspaces. When migrating between backends, Terraform will copy all
  workspaces (with the same names). THIS WILL OVERWRITE any conflicting
  states in the destination.

  Terraform initialization doesn't currently migrate only select workspaces.
  If you want to migrate a select number of workspaces, you must manually
  pull and push those states.

  If you answer "yes", Terraform will migrate all states. If you answer
  "no", Terraform will abort.

  Enter a value: yes


Successfully configured the backend "s3"! Terraform will automatically
use this backend unless the backend configuration changes.
```

[DynamoDB locking](https://github.com/hashicorp/terraform/blob/v0.11.7/backend/remote-state/s3/client.go)

```go
putParams := &dynamodb.PutItemInput{
	Item: map[string]*dynamodb.AttributeValue{
		"LockID": {S: aws.String(c.lockPath())},
		"Info":   {S: aws.String(string(info.Marshal()))},
	},
	TableName:           aws.String(c.ddbTable),
	ConditionExpression: aws.String("attribute_not_exists(LockID)"),
}
_, err := c.dynClient.PutItem(putParams)
```

## Static Website

## Travis Job

![travis environment variables](https://thepracticaldev.s3.amazonaws.com/i/e6robyfmw80kvoblmfmv.png)

Travis build: https://travis-ci.org/FRosner/aws_travis/builds/399020823

![website deployed](https://thepracticaldev.s3.amazonaws.com/i/vpkrnboq718ax4074it1.png)

# Conclusion

# Links

- https://www.terraform.io/docs/backends/types/s3.html#dynamodb_table
